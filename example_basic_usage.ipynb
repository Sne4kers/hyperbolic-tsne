{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HyperbolicTSNE\n",
    "\n",
    "This notebook illustrates the usage of the HyperbolicTSNE library. Specifically, we load a subset of the MNIST dataset and embed it in hyperbolic space using the accelerated version of hyperbolic tsne. Finally, we save the embedding result as an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, we import the packages we will use and set important paths. Note that `hyperbolicTSNE.util` and `hyperbolicTSNE.visualization` contain useful functions for reading, processing and exporting embeddings. This requires that hyperbolicTSNE has been set up as detailed in the main readme of the repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:18:09.224771636Z",
     "start_time": "2023-11-13T15:18:08.516888492Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import traceback\n",
    "\n",
    "from hyperbolicTSNE.util import find_last_embedding\n",
    "from hyperbolicTSNE.visualization import plot_poincare, animate\n",
    "from hyperbolicTSNE import load_data, Datasets, SequentialOptimizer, initialization, HyperbolicTSNE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "We assume that there is a top-level folder `datasets` that holds the MNIST data set. Refer to the main readme of the repository for where to find the data sets used in this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:20:09.564819212Z",
     "start_time": "2023-11-13T15:20:09.560922527Z"
    }
   },
   "outputs": [],
   "source": [
    "data_home = \"datasets\"\n",
    "log_path = \"temp/poincare/\"  # path for saving embedding snapshots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure\n",
    "\n",
    "HyperbolicTSNE follows a similar API to other t-SNE libraries like OpenTSNE and sklearn. The configuration process consists of loading the data to embed and defining the settings of the embedder. We create a dict with parameters manually to demonstrate all the customization options. Nevertheless, `hyperbolicTSNE.hyperbolicTSNE` provides parameter templates to start with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:25:10.788279509Z",
     "start_time": "2023-11-13T15:25:07.900874773Z"
    }
   },
   "outputs": [],
   "source": [
    "only_animate = False\n",
    "seed = 42\n",
    "dataset = Datasets.C_ELEGANS  # the Datasets handler provides access to several data sets used throughout the repository\n",
    "num_points = 10000  # we use a subset for demonstration purposes, full MNIST has N=70000\n",
    "perp = 30  # we use a perplexity of 30 in this example\n",
    "\n",
    "dataX, dataLabels, D, V, _ = load_data(\n",
    "    dataset, \n",
    "    data_home=data_home, \n",
    "    random_state=seed, \n",
    "    to_return=\"X_labels_D_V\",\n",
    "    hd_params={\"perplexity\": perp}, \n",
    "    sample=num_points, \n",
    "    knn_method=\"hnswlib\"  # we use an approximation of high-dimensional neighbors to speed up computations\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:37:10.158602643Z",
     "start_time": "2023-11-13T15:37:10.142279339Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please note that `empty_sequence` uses the KL divergence with Barnes-Hut approximation (angle=0.5) by default.\n",
      "config: {'learning_rate_ex': 0.8333333333333334, 'learning_rate_main': 0.8333333333333334, 'exaggeration': 12, 'exaggeration_its': 250, 'gradientDescent_its': 750, 'vanilla': False, 'momentum_ex': 0.5, 'momentum': 0.8, 'exact': False, 'area_split': False, 'n_iter_check': 10, 'size_tol': 0.999}\n"
     ]
    }
   ],
   "source": [
    "exaggeration_factor = 12  # Just like regular t-SNE, we use early exaggeration with a factor of 12\n",
    "learning_rate = (dataX.shape[0] * 1) / (exaggeration_factor * 1000)  # We adjust the learning rate to the hyperbolic setting\n",
    "ex_iterations = 250  # The embedder is to execute 250 iterations of early exaggeration, ...\n",
    "main_iterations = 750  # ... followed by 750 iterations of non-exaggerated gradient descent.\n",
    "\n",
    "opt_config = dict(\n",
    "    learning_rate_ex=learning_rate,  # learning rate during exaggeration\n",
    "    learning_rate_main=learning_rate,  # learning rate main optimization \n",
    "    exaggeration=exaggeration_factor, \n",
    "    exaggeration_its=ex_iterations, \n",
    "    gradientDescent_its=main_iterations, \n",
    "    vanilla=False,  # if vanilla is set to true, regular gradient descent without any modifications is performed; for  vanilla set to false, the optimization makes use of momentum and gains\n",
    "    momentum_ex=0.5,  # Set momentum during early exaggeration to 0.5\n",
    "    momentum=0.8,  # Set momentum during non-exaggerated gradient descent to 0.8\n",
    "    exact=False,  # To use the quad tree for acceleration (like Barnes-Hut in the Euclidean setting) or to evaluate the gradient exactly\n",
    "    area_split=False,  # To build or not build the polar quad tree based on equal area splitting or - alternatively - on equal length splitting\n",
    "    n_iter_check=10,  # Needed for early stopping criterion\n",
    "    size_tol=0.999  # Size of the embedding to be used as early stopping criterion\n",
    ")\n",
    "\n",
    "opt_params = SequentialOptimizer.sequence_poincare(**opt_config)\n",
    "\n",
    "# Start: configure logging\n",
    "logging_dict = {\n",
    "    \"log_path\": log_path\n",
    "}\n",
    "opt_params[\"logging_dict\"] = logging_dict\n",
    "\n",
    "log_path = opt_params[\"logging_dict\"][\"log_path\"]\n",
    "# Delete old log path\n",
    "if os.path.exists(log_path) and not only_animate:\n",
    "    import shutil\n",
    "    shutil.rmtree(log_path)\n",
    "# End: logging\n",
    "\n",
    "print(f\"config: {opt_config}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run HyperbolicTSNE\n",
    "\n",
    "Embedding the high dimensional data consists of three steps:\n",
    "- Initializating the embedding\n",
    "- Initializing the embedder \n",
    "- Embedding the data\n",
    "\n",
    "The following three cells demonstrate this process. Note that use set metric to \"precomputed\" because we pass the distance matrix to the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:37:39.820114839Z",
     "start_time": "2023-11-13T15:37:39.690680821Z"
    }
   },
   "outputs": [],
   "source": [
    "# Compute an initial embedding of the data via PCA\n",
    "X_embedded = initialization(\n",
    "    n_samples=dataX.shape[0],\n",
    "    n_components=2,\n",
    "    X=dataX,\n",
    "    random_state=seed,\n",
    "    method=\"pca\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:38:18.116323976Z",
     "start_time": "2023-11-13T15:38:18.075397174Z"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the embedder\n",
    "htsne = HyperbolicTSNE(\n",
    "    init=X_embedded, \n",
    "    n_components=2, \n",
    "    metric=\"precomputed\", \n",
    "    verbose=True, \n",
    "    opt_method=SequentialOptimizer, \n",
    "    opt_params=opt_params\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:40:06.268727695Z",
     "start_time": "2023-11-13T15:38:58.361025102Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[HyperbolicTSNE] Received iterable as input. It should have len=2 and contain (D=None, V=None)\n",
      "[hd_mat] Warning: There is nothing to do with given parameters. Returning given D and V\n",
      "Running Gradient Descent, Verbosity: True\n",
      "[gradient_descent] Warning: because of logging, the cf will be computed at every iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gradient Descent error: 57.60865 grad_norm: 7.06821e-01: 100%|██████████████████████████████████████████████████████████████████████████████████████████| 250/250 [00:24<00:00, 10.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Gradient Descent, Verbosity: True\n",
      "[gradient_descent] Warning: because of logging, the cf will be computed at every iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gradient Descent error: 1.85786 grad_norm: 5.98902e-01:   6%|█████▎                                                                                      | 43/750 [00:04<01:01, 11.54it/s]"
     ]
    }
   ],
   "source": [
    "# Compute the embedding\n",
    "try:\n",
    "    hyperbolicEmbedding = htsne.fit_transform((D, V))\n",
    "except ValueError:\n",
    "    hyperbolicEmbedding = find_last_embedding(log_path)\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting and visualization\n",
    "\n",
    "After running the embedding process, the embeddings arrays are saved to the `log_path`. We can use this information to visualize the embeddings using utility functions defined in `hyperbolicTSNE.visualization` as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:43:29.644001757Z",
     "start_time": "2023-11-13T15:43:29.408874430Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a rendering of the embedding and save it to a file\n",
    "if not os.path.exists(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "fig = plot_poincare(hyperbolicEmbedding, dataLabels)\n",
    "fig.savefig(f\"results/{dataset.name}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-13T15:49:11.530204393Z",
     "start_time": "2023-11-13T15:49:04.075561907Z"
    }
   },
   "outputs": [],
   "source": [
    "# This renders a GIF animation of the embedding process. If FFMPEG is installed, the command also supports .mp4 as file ending \n",
    "animate(logging_dict, dataLabels, f\"results/{dataset.name}_ani.gif\", fast=True, plot_ee=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "htsne",
   "language": "python",
   "name": "pyenv_htsne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
